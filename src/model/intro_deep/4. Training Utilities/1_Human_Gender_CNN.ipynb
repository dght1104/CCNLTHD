{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"anaconda-cloud":{},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11163851,"sourceType":"datasetVersion","datasetId":6966408}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview\n- ModelCheckpoint: save model while training\n- EarlyStopping: stop training early to avoid overfitting\n- ReduceLROnPlateau: reduce learning rate while training\n- TensorBoard: plot training and validation metrics","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import keras\nfrom keras.models import Model\nfrom keras.layers import Input, Conv2D, MaxPooling2D, Flatten, Dense, Dropout\nfrom keras.optimizers import Adam\n\n# Training utilities\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard\n\nfrom IPython.display import SVG\nfrom tensorflow.keras.utils import model_to_dot, plot_model\n\nimport time\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:34:15.854091Z","iopub.execute_input":"2025-03-31T01:34:15.854339Z","iopub.status.idle":"2025-03-31T01:34:35.686573Z","shell.execute_reply.started":"2025-03-31T01:34:15.854304Z","shell.execute_reply":"2025-03-31T01:34:35.685934Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_classes = 2   # male vs. female","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:34:35.687346Z","iopub.execute_input":"2025-03-31T01:34:35.687939Z","iopub.status.idle":"2025-03-31T01:34:35.691302Z","shell.execute_reply.started":"2025-03-31T01:34:35.687913Z","shell.execute_reply":"2025-03-31T01:34:35.690620Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (1) Data","metadata":{}},{"cell_type":"markdown","source":"### Download data at https://drive.google.com/drive/folders/0BxINLo5jshCRYW8xODhNSlkyLTQ?resourcekey=0-RNJvvSSE67Q1bMqQi3A1RQ&usp=sharing\nThis source code and npy files MUST be in the same location","metadata":{}},{"cell_type":"code","source":"x_train = np.load('/kaggle/input/human-gender1/64_64_11938_4098_train_x_onehot.npy')\ny_train = np.load('/kaggle/input/human-gender1/64_64_11938_4098_train_y_onehot.npy')\nx_train = np.rot90(x_train, k=3, axes=(1, 2))\n\nx_test = np.load('/kaggle/input/human-gender1/64_64_5968_4098_val_x_onehot.npy')\ny_test = np.load('/kaggle/input/human-gender1/64_64_5968_4098_val_y_onehot.npy')\nx_test = np.rot90(x_test, k=3, axes=(1, 2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:34:35.692387Z","iopub.execute_input":"2025-03-31T01:34:35.692725Z","iopub.status.idle":"2025-03-31T01:34:39.253584Z","shell.execute_reply.started":"2025-03-31T01:34:35.692691Z","shell.execute_reply":"2025-03-31T01:34:39.252601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (2) Declare model","metadata":{}},{"cell_type":"code","source":"# 64x64 portray image\ninput_image = Input(shape=(64, 64, 1), name='Input')\n\n# conv, pooling layers + dropout\nx = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', name='conv1_1')(input_image)\nx = Conv2D(filters=16, kernel_size=(3, 3), padding='same', activation='relu', name='conv1_2')(x)\nx = MaxPooling2D(pool_size=(2, 2), name='pool1')(x)\nx = Dropout(rate=0.2, name='conv_dropout1')(x)\n\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', name='conv2_1')(x)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', name='conv2_2')(x)\nx = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', name='conv2_3')(x)\nx = MaxPooling2D(pool_size=(2, 2), name='pool2')(x)\nx = Dropout(rate=0.2, name='conv_dropout2')(x)\n\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', name='conv3_1')(x)\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', name='conv3_2')(x)\nx = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu', name='conv3_3')(x)\nx = MaxPooling2D(pool_size=(2, 2), name='pool3')(x)\nx = Dropout(rate=0.2, name='conv_dropout3')(x)\n\nx = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu', name='conv4_1')(x)\nx = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu', name='conv4_2')(x)\nx = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu', name='conv4_3')(x)\nx = MaxPooling2D(pool_size=(2, 2), name='pool4')(x)\nx = Dropout(rate=0.2, name='conv_dropout4')(x)\n\nx = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu', name='conv5_1')(x)\nx = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu', name='conv5_2')(x)\nx = Conv2D(filters=256, kernel_size=(3, 3), padding='same', activation='relu', name='conv5_3')(x)\nx = MaxPooling2D(pool_size=(2, 2), name='pool5')(x)\nx = Dropout(rate=0.2, name='conv_dropout5')(x)\n\n# similar to the MLP example!\n# matrix ---> vector\nx = Flatten(name='flatten')(x)\n\n# FC layers + dropout\nx = Dense(units=1024, activation='relu', name='fc1')(x)\nx = Dropout(rate=0.2, name='fc_dropout1')(x)\n\nx = Dense(units=1024, activation='relu', name='fc2')(x)\nx = Dropout(rate=0.2, name='fc_dropout2')(x)\n\noutput_label = Dense(units=num_classes, activation='softmax', name='fc3_10ways_softmax')(x)\n\n# define model\nmodel = Model(inputs=input_image, outputs=output_label, name='mnist_mlp')\n\n# print model summary\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:34:39.255826Z","iopub.execute_input":"2025-03-31T01:34:39.256085Z","iopub.status.idle":"2025-03-31T01:34:43.272508Z","shell.execute_reply.started":"2025-03-31T01:34:39.256063Z","shell.execute_reply":"2025-03-31T01:34:43.271820Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SVG(model_to_dot(model, show_shapes=True).create(prog='dot', format='svg'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:34:43.273715Z","iopub.execute_input":"2025-03-31T01:34:43.273986Z","iopub.status.idle":"2025-03-31T01:34:43.840027Z","shell.execute_reply.started":"2025-03-31T01:34:43.273962Z","shell.execute_reply":"2025-03-31T01:34:43.839196Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (3) Train defined model\n- Note that the training history including loss and accuracy will be save in 'history' variable\n- In case your system runs out of memory (OOM), try to decrease batch size","metadata":{}},{"cell_type":"code","source":"# Khai báo learning rate, loss function, và metric\nloss = 'categorical_crossentropy'\nlearning_rate = 0.001\nmodel.compile(loss=loss, optimizer=Adam(learning_rate=learning_rate), metrics=['accuracy'])\n\n# Train model\nbatch_size = 128\nepochs = 100000  # Sử dụng EarlyStopping nên có thể đặt lớn\n\n# ===============================\n#      Training utilities\n# ===============================\n# ModelCheckpoint: Lưu model tốt nhất\ncheck_pointer = ModelCheckpoint(\n    filepath='best_model.keras',  # Lưu cố định file model tốt nhất\n    monitor='val_loss',\n    save_best_only=True,\n    mode='min'\n)\n\n# EarlyStopping: Dừng sớm nếu không cải thiện\nearly_stopping_after_epochs = 5\nearly_stopping = EarlyStopping(monitor='val_loss', patience=early_stopping_after_epochs, mode='min')\n\n# ReduceLROnPlateau: Giảm learning rate nếu không cải thiện\nreduce_lr_after_epochs = 5\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=reduce_lr_after_epochs, min_lr=0.0001, mode='min')\n\n# TensorBoard để hiển thị log\ntf_board = TensorBoard(log_dir='./logs')\n\n# Huấn luyện mô hình\nstarting_time = time.time()\nmodel.fit(x_train, y_train,\n          validation_data=(x_test, y_test),\n          batch_size=batch_size,\n          epochs=epochs,\n          callbacks=[check_pointer, early_stopping, reduce_lr, tf_board])\n\nprint('> Training time is %.4f minutes' % ((time.time() - starting_time) / 60))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T01:34:43.841020Z","iopub.execute_input":"2025-03-31T01:34:43.841281Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Training time\n- Training time with GTX 1080 is about 3.1879 minutes for 32 epochs\n- Validation accuracy is ~87.7% at epoch #27. Without training utilities, the best accuracy should be ~82.6%","metadata":{}},{"cell_type":"markdown","source":"### Load trained model and evaluate it again with validation set","metadata":{}},{"cell_type":"code","source":"model.load_weights('best_model.keras')\nmodel.evaluate(x_test, y_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checkpoint files","metadata":{}},{"cell_type":"code","source":"cp = plt.imread('checkpoints.PNG')\nplt.figure(figsize=(15, 10))\nplt.imshow(cp)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### TensorBoard","metadata":{}},{"cell_type":"code","source":"cp = plt.imread('tfboard.PNG')\nplt.figure(figsize=(16, 13))\nplt.imshow(cp)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# (4) Evaluate trained model","metadata":{}},{"cell_type":"code","source":"score = model.evaluate(x_test, y_test)\nprint('Test loss:', score[0])\nprint('Test accuracy:', score[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def int2gender(num):\n    if num == 0:\n        return 'female'\n    else:\n        return 'male'","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# select a test image randomly\nrandom_test_index = np.random.choice(x_test.shape[0], size=1)[0]\ntest_img = x_test[random_test_index]\ntest_label = np.argmax(y_test[random_test_index])\n\n# predict test image with trained model\npred_label = model.predict(np.expand_dims(test_img, axis=0))\npred_label = np.argmax(pred_label)\n\nplt.imshow(test_img[:, :, 0], cmap='gray')\nplt.title('true label = %s, predicted label = %s' % (int2gender(test_label), int2gender(pred_label)))\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}